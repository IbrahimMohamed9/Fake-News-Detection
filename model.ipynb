{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8355efc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clement_Bisaillon\n",
    "# title,text,subject,date\n",
    "import pandas as pd\n",
    "\n",
    "def convert_to_json(file_path_csv, label):\n",
    "    df = pd.read_csv(file_path_csv, engine=\"python\")\n",
    "    df.drop([\"subject\",\"date\"], axis=1, inplace=True)\n",
    "    df[\"label\"] = label\n",
    "    return df\n",
    "\n",
    "db_name = \"Clement_Bisaillon\"\n",
    "base_file_path = f\"./data/{db_name}/\"\n",
    "\n",
    "pdfake = convert_to_json(base_file_path + \"Fake.csv\",  0)\n",
    "pdtrue = convert_to_json(base_file_path + \"True.csv\", 1)\n",
    "\n",
    "pdcombined = pd.concat([pdfake, pdtrue], ignore_index=True)\n",
    "pdcombined.to_json(f\"./data/jsons/{db_name}.json\", orient=\"records\", indent=2, force_ascii=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1e022b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erfan_Moosavi_Monazzah\n",
    "# title\ttext\tsubject\tdate\tlabel\n",
    "import pandas as pd\n",
    "\n",
    "db_name = \"Erfan_Moosavi_Monazzah\"\n",
    "\n",
    "base_file_path = f\"./data/{db_name}/\"\n",
    "\n",
    "df = pd.read_csv(base_file_path + \"test.tsv\", sep=\"\\t\")\n",
    "\n",
    "df.drop([\"subject\",\"date\", \"Unnamed: 0\"], axis=1, inplace=True)\n",
    "\n",
    "df.to_json(f\"./data/jsons/{db_name}.json\", orient=\"records\", indent=2, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35adf368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mahdi_Mashayekhi\n",
    "# title,text,date,source,author,category,label\n",
    "import pandas as pd\n",
    "\n",
    "db_name = \"Mahdi_Mashayekhi\"\n",
    "\n",
    "base_file_path = f\"./data/{db_name}/\"\n",
    "df = pd.read_csv(base_file_path + \"fake_news_dataset.csv\", engine=\"python\")\n",
    "df.drop([\"date\",\"source\",\"author\",\"category\"], axis=1, inplace=True)\n",
    "df[\"label\"] = df[\"label\"].map({\"real\": 1, \"fake\": 0})\n",
    "\n",
    "df.to_json(f\"./data/jsons/{db_name}.json\", orient=\"records\", indent=2, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d102862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saurabh_Shahane\n",
    "# ,title,text,label\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "csv.field_size_limit(10**7) \n",
    "\n",
    "db_name = \"Saurabh_Shahane\"\n",
    "base_file_path = f\"./data/{db_name}/\"\n",
    "\n",
    "df = pd.read_csv(base_file_path + \"WELFake_Dataset.csv\", engine=\"python\")\n",
    "df.drop([\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "\n",
    "df.to_json(f\"./data/jsons/{db_name}.json\", orient=\"records\", indent=2, force_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4b8c10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subhajournal\n",
    "# ,title,text,label\n",
    "import pandas as pd\n",
    "\n",
    "db_name = \"Subhajournal\"\n",
    "base_file_path = f\"./data/{db_name}/\"\n",
    "df = pd.read_csv(base_file_path + \"Fake_Real_News_Data.csv\", engine=\"python\")\n",
    "\n",
    "df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "\n",
    "df[\"label\"] = df[\"label\"].map({\"FAKE\": 0, \"REAL\": 1})\n",
    "\n",
    "df.to_json(f\"./data/jsons/{db_name}.json\", orient=\"records\", indent=2, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "800fb0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 151634\n",
      "Saved to: ./combined_news.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_folder = \"./jsons/\"\n",
    "output_file = \"./combined_news.json\"\n",
    "\n",
    "dfs_dirs = [\"Clement_Bisaillon.json\",\n",
    "        \"Erfan_Moosavi_Monazzah.json\",\n",
    "        \"Mahdi_Mashayekhi.json\",\n",
    "        \"Saurabh_Shahane.json\",\n",
    "        \"Subhajournal.json\"]\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for file in dfs_dirs:\n",
    "    df = pd.read_json(f\"./data/jsons/{file}\")\n",
    "    dfs.append(df)\n",
    "\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(\"Total rows:\", combined_df.shape[0])\n",
    "combined_df.to_json(output_file, orient=\"records\", indent=2, force_ascii=False)\n",
    "print(\"Saved to:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbde9477",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ibrah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ibrah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Ibrah\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# pip install nltk\n",
    "\n",
    "# --------------------------\n",
    "# NLTK setup (make sure you downloaded these once):\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "# --------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41394b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['title', 'text', 'label']\n",
      "Records: 151634\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "contractions = {\n",
    "    \"i'm\": \"i am\", \"im\": \"i am\",\n",
    "    \"i'll\": \"i will\", \"ill\": \"i will\",\n",
    "    \"i've\": \"i have\", \"ive\": \"i have\",\n",
    "\n",
    "    \"can't\": \"cannot\", \"cant\": \"cannot\",\n",
    "    \"aren't\": \"are not\", \"arent\": \"are not\",\n",
    "    \"couldn't\": \"could not\", \"couldnt\": \"could not\",\n",
    "    \"didn't\": \"did not\", \"didnt\": \"did not\",\n",
    "    \"doesn't\": \"does not\", \"doesnt\": \"does not\",\n",
    "    \"hadn't\": \"had not\", \"hadnt\": \"had not\",\n",
    "    \"hasn't\": \"has not\", \"hasnt\": \"has not\",\n",
    "    \"haven't\": \"have not\", \"havent\": \"have not\",\n",
    "    \"isn't\": \"is not\", \"isnt\": \"is not\",\n",
    "    \"let's\": \"let us\", \"lets\": \"let us\",\n",
    "    \"mustn't\": \"must not\", \"mustnt\": \"must not\",\n",
    "    \"won't\": \"will not\", \"wont\": \"will not\",\n",
    "    \"don't\": \"do not\", \"dont\": \"do not\",\n",
    "\n",
    "    \"you're\": \"you are\", \"youre\": \"you are\",\n",
    "    \"they're\": \"they are\", \"theyre\": \"they are\",\n",
    "    \"she's\": \"she is\", \"shes\": \"she is\",\n",
    "    \"he's\": \"he is\", \"hes\": \"he is\",\n",
    "    \"we're\": \"we are\", \"were\": \"we are\",\n",
    "    \"that's\": \"that is\", \"thats\": \"that is\",\n",
    "    \"it's\": \"it is\", \"its\": \"it is\",\n",
    "}\n",
    "\n",
    "def expand_contractions(text):\n",
    "    for c, full in contractions.items():\n",
    "        text = re.sub(r\"\\b\" + re.escape(c) + r\"\\b\", full, text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "def normalize_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\" if text is None else str(text)\n",
    "\n",
    "    text = text.lower()\n",
    "    text = expand_contractions(text)\n",
    "    # remove punctuation (keeps letters/digits/underscore)\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
    "\n",
    "    tokens = text.split()\n",
    "\n",
    "    # remove stop words + lemmatize\n",
    "    cleaned_tokens = [\n",
    "        lemmatizer.lemmatize(tok)\n",
    "        for tok in tokens\n",
    "        if tok not in stop_words\n",
    "    ]\n",
    "\n",
    "    return \" \".join(cleaned_tokens)\n",
    "\n",
    "def clean_news(in_path, out_path = \"cleaned_news.json\"):\n",
    "    df = pd.read_json(in_path)\n",
    "\n",
    "    print(\"Columns:\", list(df.columns))\n",
    "    print(\"Records:\", len(df))\n",
    "\n",
    "    df[\"title\"] = df[\"title\"].apply(normalize_text)\n",
    "    df[\"text\"] = df[\"text\"].apply(normalize_text)\n",
    "\n",
    "    df.to_json(out_path, orient=\"records\", indent=2, force_ascii=False)\n",
    "\n",
    "    print(f\"Cleaned JSON written to: {out_path}\")\n",
    "\n",
    "clean_news(\"combined_news.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7522ced6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
